# -*- coding: utf-8 -*-
"""AstraDB_langchain_quickstart_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/datastaxdevs/mini-demo-astradb-langchain/blob/main/AstraDB_langchain_quickstart_1.ipynb

# Integrate LangChain with Astra DB Serverless
"""

!pip install "langchain>=0.2" "langchain-astradb>=0.4" \
    "langchain-openai>=0.1" "datasets>=3.0" "pypdf>=5.0" \
    "python-dotenv>=1.0"

"""## Secrets

Example values:
- API Endpoint: `"https://01234567-89ab-cdef-0123-456789abcdef-us-east1.apps.astra.datastax.com"`
- Token: `"AstraCS:6gBhNmsk135..."` (it must have a role of at least "Database Administrator")
- OpenAI API key: `sk-4fQ3F...`
"""

import os
from getpass import getpass
os.environ["ASTRA_DB_APPLICATION_TOKEN"] = getpass("ASTRA_DB_APPLICATION_TOKEN = ")
os.environ["ASTRA_DB_API_ENDPOINT"] = input("ASTRA_DB_API_ENDPOINT = ")
if _desired_keyspace := input("ASTRA_DB_KEYSPACE (optional) = "):
    os.environ["ASTRA_DB_KEYSPACE"] = _desired_keyspace
os.environ["OPENAI_API_KEY"] = getpass("OPENAI_API_KEY = ")

"""## Dependencies"""

from langchain_astradb import AstraDBVectorStore
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

from datasets import load_dataset

ASTRA_DB_APPLICATION_TOKEN = os.environ["ASTRA_DB_APPLICATION_TOKEN"]
ASTRA_DB_API_ENDPOINT = os.environ["ASTRA_DB_API_ENDPOINT"]
ASTRA_DB_KEYSPACE = os.environ.get("ASTRA_DB_KEYSPACE")
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

"""## Create the embeddings model and vector store with its collection"""

embedding = OpenAIEmbeddings()
vstore = AstraDBVectorStore(
    collection_name="test",
    embedding=embedding,
    token=ASTRA_DB_APPLICATION_TOKEN,
    api_endpoint=ASTRA_DB_API_ENDPOINT,
    namespace=ASTRA_DB_KEYSPACE,
)

"""## Load a small dataset"""

philo_dataset = load_dataset("datastax/philosopher-quotes")["train"]
print("An example entry:")
print(philo_dataset[16])

"""## Transform into LangChain "Documents"
"""

docs = []
for entry in philo_dataset:
    metadata = {"author": entry["author"]}
    if entry["tags"]:
        # Add metadata tags to the metadata dictionary
        for tag in entry["tags"].split(";"):
            metadata[tag] = "y"
    # Add a LangChain document with the quote and metadata tags
    doc = Document(page_content=entry["quote"], metadata=metadata)
    docs.append(doc)

"""## Compute vector embedding and store entries"""

inserted_ids = vstore.add_documents(docs)
print(f"\nInserted {len(inserted_ids)} documents.")

"""## Run a similarity search (to verify the integration)"""

results = vstore.similarity_search("Our life is what we make of it", k=3)
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")

"""## Further usage patterns

### Use `add_texts`
"""

texts = [
    "I think, therefore I am.",
    "To the things themselves!",
]
metadatas = [
    {"author": "descartes", "knowledge": "y"},
    {"author": "husserl", "knowledge": "y"},
]
ids = [
    "desc_01",
    "huss_xy",
]
inserted_ids_2 = vstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)
print(f"\nInserted {len(inserted_ids_2)} documents.")

"""### Return similarity scores from a search"""

results = vstore.similarity_search_with_score("Our life is what we make of it", k=3)
for res, score in results:
    print(f"* [SIM={score:3f}] {res.page_content} [{res.metadata}]")

"""### Similarity search with metadata filtering"""

results = vstore.similarity_search(
    "Our life is what we make of it",
    k=3,
    filter={"author": "aristotle"},
)
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")

"""### MMR (maximal marginal relevance) similarity search"""

results = vstore.max_marginal_relevance_search(
    "Our life is what we make of it",
    k=3,
    filter={"author": "aristotle"},
)
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")

"""### Deleting documents from the store

#### Delete by document ID
"""

delete_1 = vstore.delete(inserted_ids[:3])
print(f"delete result = {delete_1}")

delete_2 = vstore.delete(inserted_ids[2:5])
print(f"delete result = {delete_2}")

"""### Retrieve and then delete

"""

ids_to_delete = []
for res_doc, res_score, res_id in vstore.similarity_search_with_score_id(
    "Philosophy has no goals",
    k=2,
):
    print(f"* [SIM={res_score:3f}] {res_doc.page_content} [{res_doc.metadata}]")
    ids_to_delete.append(res_id)

print(f"Deleting IDs = {ids_to_delete} ...")
success = vstore.delete(ids_to_delete)
print(f"Deletion succeeded = {success}")

"""Now try again the same search:"""

for res_doc, res_score, res_id in vstore.similarity_search_with_score_id(
    "Philosophy has no goals",
    k=2,
):
    print(f"* [SIM={res_score:3f}] {res_doc.page_content} [{res_doc.metadata}]")

vstore.clear()

"""## Cleanup"""

vstore.delete_collection()